---
title: Logistic回归
date: 2019-6-10 21:58:02
categories: 技术储备
tags: [机器学习]
mathjax: true
---

----

<!-- more -->

# 1. 介绍

## 1.1 什么是回归？

假设现在有一些数据点,我们用一条直线对这些点进行拟合(该线称为最佳拟合直线),这个拟合过程就称为回归.

## 1.2 主要思想

利用Logistic回归进行分类的主要思想:根据现有数据对分类边界线建立回归公式,以此进行分类.

## 1.3 训练分类器的实质

寻找最佳的拟合参数,使用的是最优化算法.

## 1.4 Logistic回归的一般过程

1. 收集数据:采用任意方法收集数据.
2. 准备数据:由于需要进行距离计算,因此要求数据类型为数值型.另外,结构化数据格式则最佳.
3. 分析数据:采用任意方法对数据进行分析.
4. 训练算法:大部分时间将用于训练,训练的目的是为了找到最佳的分类回归系数.
5. 测试算法:一旦训练步骤完成,分类将会很快.
6. 使用算法:首先,我们需要输入一些数据,并将其转换成对应的结构化数值;接着,基于训练好的回归系数就可以对这些数值进行简单回归计算,判定它们属于哪个类别;在这之后,我们就可以在输出的类别上做一些其他分析工作.

## 1.5 Logistic回归的优缺点

1. 优点:计算代价不高,易于理解和实现.
2. 缺点:容易欠拟合,分类精度可能不高.

# 2. 原理

## 2.1 Sigmoid函数

我们想要的函数应该是,能接受所有的输入然后预测出类别.
例如,在两个类的情况下,上述函数输出0或1,该函数称为**海维塞德阶跃函数**,或者直接称为**单位阶跃函数**.
这类函数的问题在于:该函数在跳跃点上从0瞬间跳跃到1,这个瞬间跳跃过程有时很难处理.
幸好,另一个函数有类似的性质,且数学上更易处理,这就是**Sigmoid函数**.
$$
σ(z)=\frac{1}{1+e^{-z}}
$$
Sigmoid函数的输入记为z,由下面的公式得出:
$$
z=w_{0}x_{0}+w_{1}x_{1}+w_{2}x_{2}+...+w_{n}x_{n}
$$
采用向量的写法,上述公式写成
$$
z=W^{T}X
$$
它表示将这两个数值向量对应元素相乘然后全部加起来即得到z值.其中的向量X是分类器的输入数据,向量W也就是我们要找到的最佳参数(系数),从而使得分类器尽可能地精确.

## 2.2 梯度上升法

梯度上升法是一种最优化算法.
梯度上升法基本思想是要找到某函数的最大值,最好的方法是沿着改函数的梯度方向探寻.如果梯度记为$\nabla$,则函数f(x,y)的梯度由下式表示:

$$\nabla f(x,y)=
	\left(\begin{array}{c}
		\frac{\partial f(x,y)}{\partial x} \\ \\
		\frac{\partial f(x,y)}{\partial y} 
	\end{array}\right)
$$

梯度上升算法到达每个点后都会重新估计移动的方向.从P0开始,计算完该点的梯度,函数就根据梯度移动到下一点P1.在P1点,梯度再次被重新计算,并沿新的梯度方向移动到P2.如此循环迭代,直到满足停止条件.迭代的过程中,梯度算子总是保证我们能选取到最佳的移动方向.

![梯度上升法](https://blogpictures-1257055754.cos.ap-guangzhou.myqcloud.com/20171231202110902.png)

可以看到,梯度算子总是指向函数值增长最快的方向.这里所说的是移动方向,而未提到移动量的大小.该量值称为步长,记做α.用向量来表示的话,梯度上升算法的迭代公式如下:
$$w := w + α\nabla_wf(w)$$
该公式将一直被迭代执行,直至达到某个停止条件为止,比如迭代次数达到某个指定的值或者算法达到某个可以允许的误差范围.

### 2.2.1 梯度下降法

本质上和梯度上升法是一回事.公式为:
$$w := w - α\nabla_wf(w)$$
梯度上升算法用来求函数的最大值,而梯度下降法用来求函数的最小值.

```txt
梯度上升法的伪代码:

每个回归系数初始化为1
重复R次:
	计算整个数据集的梯度
	使用alpha x gradient更新回归系数的向量
返回回归系数
```

```python
def gradAscent(dataMatIn, classLabels):
    dataMatrix = mat(dataMatIn)             
    labelMat = mat(classLabels).transpose() 
    m,n = shape(dataMatrix)
    alpha = 0.001
    maxCycles = 500
    weights = ones((n,1))
    for k in range(maxCycles):              
        h = sigmoid(dataMatrix*weights)     
        error = (labelMat - h)              
        weights = weights + alpha * dataMatrix.transpose()* error 
    return weights
```

## 2.3 随机梯度上升

梯度上升法在每次更新回归系数时都需要遍历整个数据集,该方法在处理100个左右的数据集时尚可,但如果有数十亿样本和成千上万的特征,那么该方法的计算复杂度就太高了.
一种改进的方法是一次仅用一个样本点来更新回归系数,该方法称为**随机梯度上升算法**.由于可以在新的样本到来时对分类器进行增量式更新,因而随机梯度上升算法是一个在线学习算法.与“**在线学习**”相对应,一次处理所有数据被称作是“**批处理**”.

```txt
随机梯度上升算法伪代码:

所有回归系数初始化为1
对数据集中的每个样本
	计算该样本的梯度
	使用alpha x gradient更新回归系数值
返回回归系数值
```

```python
def stocGradAscent0(dataMatrix, classLabels):
    m,n = shape(dataMatrix)
    alpha = 0.01
    weights = ones(n)   
    for i in range(m):
        h = sigmoid(sum(dataMatrix[i]*weights))
        error = classLabels[i] - h
        weights = weights + alpha * error * dataMatrix[i]
    return weights
```

![这里写图片描述](https://blogpictures-1257055754.cos.ap-guangzhou.myqcloud.com/20171231213507011.png)

上图展示了随机梯度上升算法在20000次迭代过程中回归系数的变化情况.其中的系数2,也就是X2只经过5000次迭代就达到了稳定值,但是系数1和0则需要更多次迭代.另外值得注意的是,在大的波动停止后,还有一些小的周期性波动.不难理解,产生这种现象的原因是存在一些不能正确分类的样本(数据集并非线性可分),在每次迭代时,会引发系数的剧烈改变.我们期望算法能避免来回波动,从而收敛到某个值.另外收敛速度也需要更快.

### 2.3.1 改进的随机梯度上升算法

```python
def stocGradAscent1(dataMatrix, classLabels, numIter=150):
    m,n = shape(dataMatrix)
    weights = ones(n)   
    for j in range(numIter):
        dataIndex = range(m)
        for i in range(m):
            alpha = 4/(1.0+j+i)+0.0001    #apha每次迭代时需要调整 
            randIndex = int(random.uniform(0,len(dataIndex)))#随机选取更新
            h = sigmoid(sum(dataMatrix[randIndex]*weights))
            error = classLabels[randIndex] - h
            weights = weights + alpha * error * dataMatrix[randIndex]
            del(dataIndex[randIndex])
    return weights
```

主要做出了3点改进:

1. alpha每次迭代的时候需要调整;(可以缓解数据波动或者高频波动.另外,虽然alpha会随着迭代次数不断减小,但永远不会减少到0,这是因为存在常数项,必须这样做的原因是为了保证多次迭代之后新数据仍然具有一定的影响.如果要处理的问题是动态变化的,那么可以适当的加大上述常数项,来确保新的值获得更大的回归系数.另一点值得注意的是,在降低alpha的函数中,alpha每次减少1/(j+i),其中j是迭代次数,i是样本点的下标.这样当j＜max(i)时,alpha就不是严格下降的.避免参数的严格下降也常见于模拟退火算法等其他优化算法中.)
2. 随机选取样本更新.(这里通过随机选取样本来更新回归系数.这种方法将减少周期性的波动,如上图,这种方法每次随机从列表中选出一个值,然后从列表中删掉该值(再进行下一次迭代))
3. 增加了一个迭代次数作为第3个参数.(如果没给定,算法会以默认次数迭代,如果给定,那么算法将按新的参数值迭代)

![这里写图片描述](http://img.blog.csdn.net/20171231215534796?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvQnJhdmVMb3Nlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

可以看出:

1. 系数不再有之前那种周期性波动,这归功于样本随机选择机制
2. 水平轴短了很多,这说明改进后的算法收敛得更快了

# 3. 小结

1. Logistic回归的目的是寻找一个非线性函数sigmoid的最佳拟合参数,求解过程可以由最优化算法来完成.在最优化算法中,最常用的就是梯度上升算法,而梯度上升算法又可以简化为随机梯度上升算法.
2. 随机梯度上升算法和梯度上升算法的效果相当,但占用更少的计算资源.此外,随机梯度是一种在线算法,可以在数据到来时就完成参数的更新,而不需要重新读取整个数据集来进行批处理运算.
3. 机器学习的一个重要问题就是如何处理缺失数据.这个问题没有标准答案,取决于实际应用中的需求.每种方案都各有优缺点.

# 4. 参考文献

< < 机器学习实战 > >