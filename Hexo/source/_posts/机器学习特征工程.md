---
title: 机器学习特征工程
date: 2019-6-8 16:04:31
categories: 技术储备
tags: [机器学习]
mathjax: true
---

----

<!-- more -->

# 1. 特征工程

特征工程,是指用一系列工程化的方式从原始数据中筛选出更好的数据特征,以提升模型的训练效果.业内有一句广为流传的话是:数据和特征决定了机器学习的上限,而模型和算法是在逼近这个上限而已.由此可见,好的数据和特征是模型和算法发挥更大的作用的前提.特征工程通常包括数据预处理,特征选择,降维等环节.

# 2. 数据预处理

数据预处理是特征工程中最为重要的一个环节,良好的数据预处理可以使模型的训练达到事半功倍的效果.数据预处理旨在通过归一化,标准化,正则化等方式改进不完整,不一致,无法直接使用的数据.

## 2.1 归一化

### 2.1.1 MinMaxScaler

MinMaxScaler变换函数为min-max标准化,也称为离差标准化,是对原始数据的线性变换,min-max标准化方法的缺陷在当有新数据加入时,可能会导致X.max和X.min的值发生变化,需要重新计算.
$$
X_{scaled}=\frac{X-X_{.\min \left( axis=0 \right)}}{X_{.\max \left( axis=0 \right)}-X_{.\min \left( axis=0 \right)}}\cdot \left( \max -\min \right) +\min
$$

max,min是给定放缩范围的最大值和最小值,通俗地解释:

$$
\text{归一化结果}=\frac{\text{该点样本值与最小样本的差}}{\text{样本在该轴的跨度}}\cdot \text{放缩范围}+\text{放缩最小值}
$$

### 2.1.2 对数函数转换

$$
x'=\log _{10}\left( x \right)
$$

### 2.1.3 反余切函数转换

$$
x'=\text{atan}\left( x \right) \cdot \frac{2}{\pi}
$$

### 2.1.4 L1归一化

$$
x'=\frac{x}{\sum_{i=0}^n{\lVert x \rVert}}
$$

### 2.1.5 L2归一化

$$
x'=\frac{x}{\sqrt{\sum_{i=0}^n{x^2}}}
$$

## 2.2 标准化

### 2.2.1 Z-score标准差标准化

$$
x'=\frac{x-\bar{x}}{\sigma}
$$

$x$为某一具体数值, $\bar{x}$是平均值, $\sigma$是标准差, $x'$是标准化结果.

## 2.3 区间缩放法

$$
x'=\frac{x-Min}{Max-Min}
$$

上述公式中 Min 代表数据最小值,Max 代表数据最大值.

## 2.4 对定量特征二值化

定量特征二值化的核心在于设定一个阈值,大于阈值的赋值为1,小于等于阈值的赋值为0,公式表达如下:

$$
x'=\begin{cases}
    \text{1,}&      x>threshold\\
    \text{0,}&      x\leqslant threshold\\
\end{cases}
$$

## 2.5 对定性特征哑编码

当数据集的特征存在定量特征时,可以进行哑编码.

举例:现在假设给你的一个病人的病情描述,一般病情的描述包含以下几个方面,将病情严重程度划分:非常严重,严重,一般严重,轻微.现在有个病人过来了,要为他构造一个病情的特征,假设他的病情是严重,我们可以给他的哑编码是0 1 0 0,病情总共有四种情况因此使用四位来表示,第二位表示严重,这位病人是严重的病情因此将其置为 1,其余为 0.

## 2.6 数据变换

常见的数据变换有基于多项式的,基于指数函数的,基于对数函数的.

2 个特征,度为 2 的多项式转换公式如下:

$$
\left( x_1,x_2 \right) =\left( \text{1,}x_1,x_2,x_{1}^{2},x_1\cdot x_2,x_{2}^{2} \right) 
$$

## 2.4 缺失值计算

这类方法是基于统计学原理用一定的值去填充空值,从而使信息表完备化.数据挖掘中常用的有以下几种补齐方法:人工填写,特殊值填充,平均值填充,热卡填充(就近补齐),K最近邻法,使用所有可能的值填充,回归,期望值最大化方法(EM)等.

### 2.4.1 平均值填充

如果空值是数值属性,就使用该属性在其他所有对象的取值的平均值来填充该缺失的属性值.
如果空值是非数值属性,就根据统计学中的众数原理,用该属性在其他所有对象出现频率最高的值来补齐该缺失的属性值.

### 2.4.2 热卡填充(就近补齐)

对于一个包含空值的对象,热卡填充法在完整数据中找到一个与它最相似的对象,然后用这个相似对象的值来进行填充.不同的问题选用不同的标准来对相似进行判定.

### 2.4.3 K最近邻法

先根据欧式距离或相关分析来确定距离具有缺失数据样本最近的K个样本,将这K个值加权平均来估计该样本的缺失数据.

### 2.4.4 使用所有可能的值填充

这种方法是用空缺属性值的所有可能的属性取值来填充,能够得到较好的补齐效果.但是当数据量很大或者遗漏的属性值较多时,其计算的代价很大,可能的测试方案很多.

### 2.4.5 回归

基于完整的数据集,建立回归方程(模型).对于包含空值的对象,将已知属性值代入方程来估计未知属性值,以此估计值来进行填充.

### 2.4.6 期望值最大化方法(EM)

在缺失类型为随机缺失的条件下,假设模型对于完整的样本是正确的,通过观测数据的边际分布可以对未知参数进行极大似然估计.它一个重要前提:适用于大样本.有效样本的数量足够以保证ML估计值是渐近无偏的并服从正态分布.

# 3. 特征选择

不同的特征对模型的影响程度不同,我们要自动地选择出对问题重要的一些特征,移除与问题相关性不是很大的特征,这个过程就叫做特征选择.特征的选择在特征工程中十分重要,往往可以直接决定最后模型训练效果的好坏.

## 3.1 过滤式

过滤式特征选择是通过评估每个特征和结果的相关性,来对特征进行筛选,留下相关性最强的几个特征.核心思想是:先对数据集进行特征选择,然后再进行模型的训练.过滤式特征选择的优点是思路简单,往往通过Pearson相关系数法,方差选择法,互信息法等方法计算相关性,然后保留相关性最强的N个特征,就可以交给模型训练;缺点是没有考虑到特征与特征之间的相关性,从而导致模型最后的训练效果没那么好.

### 3.1.1 方差选择法

使用方差选择法,先要计算各个特征的方差,然后根据阈值,选择方差大于阈值的特征.

### 3.1.2 相关系数法

使用相关系数法,先要计算各个特征对目标值的相关系数以及相关系数的P值.

### 3.1.3 卡方检验

经典的卡方检验是检验定性自变量对定性因变量的相关性.假设自变量有N种取值,因变量有M种取值,考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距,构建统计量:

$$
\chi ^2=\sum{\frac{\left( A-E \right) ^2}{E}}
$$

### 3.1.4  互信息法

经典的互信息也是评价定性自变量对定性因变量的相关性的,互信息计算公式如下:

$$
I\left( X;Y \right) =\sum_{x\epsilon X}{\sum_{y\epsilon Y}{p\left( x,y \right) \log \frac{p\left( x,y \right)}{p\left( x \right) p\left( y \right)}}}
$$

## 3.2 包裹式

包裹式特征选择是把最终要使用的机器学习模型,评测性能的指标作为特征选择的重要依据,每次去选择若干特征,或是排除若干特征.通常包裹式特征选择要比过滤式的效果更好,但由于训练过程时间久,系统的开销也更大.最典型的包裹型算法为递归特征删除算法,其原理是使用一个基模型(如:随机森林,逻辑回归等)进行多轮训练,每轮训练结束后,消除若干权值系数较低的特征,再基于新的特征集进行新的一轮训练.

### 3.2.1 递归特征消除法

递归消除特征法使用一个基模型来进行多轮训练,每轮训练后,消除若干权值系数的特征,再基于新的特征集进行下一轮训练.

## 3.3 嵌入式

嵌入式特征选择法是根据机器学习的算法,模型来分析特征的重要性,从而选择最重要的N个特征.与包裹式特征选择法最大的不同是,嵌入式方法是将特征选择过程与模型的训练过程结合为一体,这样就可以快速地找到最佳的特征集合,更加高效,快捷.常用的嵌入式特征选择方法有基于正则化项(如:L1正则化)的特征选择法和基于树模型的特征选择法(如:GBDT).

### 3.3.1 基于惩罚项的特征选择法

使用带惩罚项的基模型,除了筛选出特征外,同时也进行了降维.

### 3.3.2 基于树模型的特征选择法

树模型中GBDT也可用来作为基模型进行特征选择,

# 4. 降维

如果拿特征选择后的数据直接进行模型的训练,由于数据的特征矩阵维度大,可能会存在数据难以理解,计算量增大,训练时间过长等问题,因此我们要对数据进行降维.降维是指把原始高维空间的特征投影到低维度的空间,进行特征的重组,以减少数据的维度.降维与特征选择最大的不同在于,特征选择是进行特征的剔除,删减,而降维是做特征的重组构成新的特征,原始特征全部"消失"了,性质发生了根本的变化.常见的降维方法有:主成分分析法(PCA)和线性判别分析法(LDA).

## 4.1 主成分分析法(PCA)

主成分分析法(PCA)是最常见的一种线性降维方法,其要尽可能在减少信息损失的前提下,将高维空间的数据映射到低维空间中表示,同时在低维空间中要最大程度上的保留原数据的特点.主成分分析法本质上是一种无监督的方法,不用考虑数据的类标,它的基本步骤大致如下:

1. 数据中心化(每个特征维度减去相应的均值)
2. 计算协方差矩阵以及它的特征值和特征向量
3. 将特征值从大到小排序并保留最上边的N个特征
4. 将高维数据转换到上述N个特征向量构成的新的空间中

在把特征映射到低维空间时要注意,每次要保证投影维度上的数据差异性最大(也就是说投影维度的方差最大).

具体参考:[主成分分析（PCA）原理详解](https://blog.csdn.net/program_developer/article/details/80632779)
备份:[主成分分析（PCA）原理详解](https://blogpictures-1257055754.cos.ap-guangzhou.myqcloud.com/PCA%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.mhtml)

## 4.2 线性判别分析法(LDA)

线性判别分析法(LDA)也是一种比较常见的线性降维方法,但不同于PCA的是,它是一种有监督的算法,也就是说它数据集的每个样本会有一个输出类标.线性判别算法的核心思想是,在把数据投影到低维空间后,希望同一种类别数据的投影点尽可能的接近,而不同类别数据的类别中心之间的距离尽可能的远.也就是说LDA是想让降维后的数据点尽可能地被区分开.

LDA步骤具体参考:[《Python机器学习》读书笔记（六）特征抽取——LDA](https://blog.csdn.net/weixin_40604987/article/details/79615968)
备份:[《Python机器学习》读书笔记（六）特征抽取——LDA](https://blogpictures-1257055754.cos.ap-guangzhou.myqcloud.com/%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96LDA.mhtml)

# 5. 参考文献

[机器学习中,有哪些特征选择的工程方法？](https://www.cnblogs.com/bonelee/p/8632866.html)
[特征工程](https://yq.aliyun.com/articles/623613)
[数据归一化 minmax_scale()函数解析](https://blog.csdn.net/Jiaach/article/details/79484990)
[特征工程（2）-数据预处理区间缩放法](https://www.deeplearn.me/1383.html)
[特征工程（3）-数据预处理归一化](https://www.deeplearn.me/1386.html)
[特征工程（5）-数据预处理哑编码](https://www.cnblogs.com/yuluoxingkong/p/9010245.html)
[特征工程（6）-数据预处理数据变换](https://www.deeplearn.me/1397.html)
[数据处理--缺失值处理&异常值处理](https://blog.csdn.net/xiedelong/article/details/81607598)
[几种常见的缺失数据插补方法](https://blog.csdn.net/zziahgf/article/details/21024167)
[主成分分析（PCA）原理详解](https://blog.csdn.net/program_developer/article/details/80632779)
[《Python机器学习》读书笔记（六）特征抽取——LDA](https://blog.csdn.net/weixin_40604987/article/details/79615968)
